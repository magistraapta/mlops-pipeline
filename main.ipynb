{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RxwTOwrMCMgx",
        "outputId": "a0efed5d-6987-409d-dfd8-28ea185b52f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tfx\n",
            "  Downloading tfx-1.15.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ml-pipelines-sdk==1.15.1 (from tfx)\n",
            "  Downloading ml_pipelines_sdk-1.15.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.0)\n",
            "Collecting ml-metadata<1.16.0,>=1.15.0 (from tfx)\n",
            "  Downloading ml_metadata-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from tfx) (24.1)\n",
            "Requirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.5.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.20.3)\n",
            "Collecting docker<5,>=4.1 (from tfx)\n",
            "  Downloading docker-4.4.4-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting google-apitools<1,>=0.5 (from tfx)\n",
            "  Downloading google_apitools-0.5.32-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting google-api-python-client<2,>=1.8 (from tfx)\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.1.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.12.2)\n",
            "Collecting apache-beam<3,>=2.47 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading apache_beam-2.58.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting attrs<24,>=19.3.0 (from tfx)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: click<9,>=7 in /usr/local/lib/python3.10/dist-packages (from tfx) (8.1.7)\n",
            "Requirement already satisfied: google-api-core<3 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.19.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.63.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.25.0)\n",
            "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.64.1)\n",
            "Collecting keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4 (from tfx)\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting kubernetes<13,>=10.0.1 (from tfx)\n",
            "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.26.4)\n",
            "Collecting pyarrow<11,>=10 (from tfx)\n",
            "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting scipy<1.13 (from tfx)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.10/dist-packages (from tfx) (6.0.2)\n",
            "Collecting tensorflow<2.16,>=2.15.0 (from tfx)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting tensorflow-hub<0.16,>=0.15.0 (from tfx)\n",
            "  Downloading tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-data-validation<1.16.0,>=1.15.1 (from tfx)\n",
            "  Downloading tensorflow_data_validation-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting tensorflow-model-analysis<0.47.0,>=0.46.0 (from tfx)\n",
            "  Downloading tensorflow_model_analysis-0.46.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tensorflow-serving-api<2.16,>=2.15 (from tfx)\n",
            "  Downloading tensorflow_serving_api-2.15.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-transform<1.16.0,>=1.15.0 (from tfx)\n",
            "  Downloading tensorflow_transform-1.15.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tfx-bsl<1.16.0,>=1.15.1 (from tfx)\n",
            "  Downloading tfx_bsl-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting fasteners<1.0,>=0.3 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (4.23.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.2.2)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (1.24.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2024.1)\n",
            "Collecting redis<6,>=5.0.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading redis-5.0.8-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2024.5.15)\n",
            "Collecting requests!=2.32.*,<3.0.0,>=2.24.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.6)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl.metadata (868 bytes)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (5.5.0)\n",
            "Collecting google-apitools<1,>=0.5 (from tfx)\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.2.0)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.23.0)\n",
            "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_pubsublite-1.11.1-py2.py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting google-cloud-storage<3,>=2.16.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.25.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.4.1)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.26.0)\n",
            "Collecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_spanner-3.48.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_dlp-3.22.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.13.4)\n",
            "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_videointelligence-2.13.5-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_recommendations_ai-0.10.12-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.16.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.8.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.63.2)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.8->tfx)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.8.2)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (0.16)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=3->tfx) (2.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=2.7.3->tfx) (2.1.5)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (3.4.1)\n",
            "Collecting kt-legacy (from keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2024.7.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (71.0.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2.0.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker<2,>=1.3.1->tfx) (5.9.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.16,>=2.15.0->tfx)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (2.4.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.16,>=2.15.0->tfx)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (0.37.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16,>=2.15.0->tfx)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tfx)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras (from keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx) (1.4.2)\n",
            "Collecting pandas<2,>=1.0 (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pyfarmhash<0.4,>=0.2.2 (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx)\n",
            "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow-metadata<1.16,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx) (1.15.0)\n",
            "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (7.7.1)\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (9.4.0)\n",
            "Collecting rouge-score<2,>=0.1.2 (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu<4,>=2.3 (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tfx) (0.44.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.6.2->tfx) (1.48.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tfx) (0.13.1)\n",
            "Collecting overrides<8.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (0.5.1)\n",
            "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.16.0->apache-beam[gcp]<3,>=2.47->tfx) (1.5.0)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.1.2)\n",
            "Collecting jedi>=0.16 (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.6.8)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.0.13)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.20.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.6.2->tfx) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.6.2->tfx) (2.20.1)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests!=2.32.*,<3.0.0,>=2.24.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests!=2.32.*,<3.0.0,>=2.24.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score<2,>=0.1.2->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.8.1)\n",
            "Collecting portalocker (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.9.0)\n",
            "Collecting colorama (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.9.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tfx) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tfx) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tfx) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tfx) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.3.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.2.13)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.5.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score<2,>=0.1.2->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.66.5)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.1.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.20.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (21.2.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.2.2)\n",
            "Downloading tfx-1.15.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_pipelines_sdk-1.15.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_beam-2.58.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_metadata-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_data_validation-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_analysis-0.46.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_serving_api-2.15.1-py2.py3-none-any.whl (26 kB)\n",
            "Downloading tensorflow_transform-1.15.0-py3-none-any.whl (451 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tfx_bsl-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading google_cloud_dlp-3.22.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_pubsublite-1.11.1-py2.py3-none-any.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.6/304.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_recommendations_ai-0.10.12-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.7/184.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_spanner-3.48.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.4/397.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_videointelligence-2.13.5-py2.py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.0/245.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_vision-3.7.4-py2.py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-5.0.8-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: google-apitools, crcmod, dill, hdfs, pyfarmhash, rouge-score, pyjsparser, docopt\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131014 sha256=b1b0e5ad1fb005f55f4c8b2e6369d9b2d16b9a6ae516172f421800cc41bd0ad0\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31405 sha256=4ef39cb7fb062b7342fa160f58e0d76b88329b8edd0503a0642d760129e7b2ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=209251a194e9140d6d26f1a2eb4c287d64093ed4d8cf13973baf3938a44b094d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=a7ecf81f7c3d75814f965a95f5e9fe176e3b5fa500421a59e687c53d2740e0d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-linux_x86_64.whl size=88659 sha256=9f14e487454eda087f315bd61712f687213fd1af7982817538d9f12ec4bf07df\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/08/da/f66b1f3258fe3f1e767b2136c5444dbfa9fa3f7944cc5e1983\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=caa913560d0c9730ccfb90e563aefbb6f998f3710dc29c0576836635df89ce04\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25983 sha256=d448bf00d32f4369b46e52e6ecb7ea9c90315eb9eb92593aaa3efa636202e02b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=fc4dba300a586b6ff57d86cbf5a1ea653567eaa7374b4cdccf28d81348b2e26a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built google-apitools crcmod dill hdfs pyfarmhash rouge-score pyjsparser docopt\n",
            "Installing collected packages: pyjsparser, pyfarmhash, kt-legacy, docopt, crcmod, zstandard, wrapt, uritemplate, tensorflow-hub, tensorflow-estimator, scipy, requests, redis, pyarrow, portalocker, overrides, orjson, objsize, ml-dtypes, keras, js2py, jedi, grpc-interceptor, fasteners, fastavro, dnspython, dill, colorama, attrs, sacrebleu, rouge-score, pymongo, pandas, ml-metadata, keras-tuner, hdfs, docker, kubernetes, google-apitools, tensorboard, google-api-python-client, tensorflow, ml-pipelines-sdk, google-cloud-vision, google-cloud-videointelligence, google-cloud-storage, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-dlp, apache-beam, tensorflow-serving-api, google-cloud-pubsublite, tfx-bsl, tensorflow-transform, tensorflow-data-validation, tensorflow-model-analysis, tfx\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.16.1\n",
            "    Uninstalling tensorflow-hub-0.16.1:\n",
            "      Successfully uninstalled tensorflow-hub-0.16.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.137.0\n",
            "    Uninstalling google-api-python-client-2.137.0:\n",
            "      Successfully uninstalled google-api-python-client-2.137.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 10.0.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "osqp 0.6.7.post0 requires scipy!=1.12.0,>=0.13.2, but you have scipy 1.12.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\n",
            "xarray 2024.6.0 requires pandas>=2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.58.1 attrs-23.2.0 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.6.1 docker-4.4.4 docopt-0.6.2 fastavro-1.9.5 fasteners-0.19 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-dlp-3.22.0 google-cloud-pubsublite-1.11.1 google-cloud-recommendations-ai-0.10.12 google-cloud-spanner-3.48.0 google-cloud-storage-2.18.2 google-cloud-videointelligence-2.13.5 google-cloud-vision-3.7.4 grpc-interceptor-0.15.4 hdfs-2.7.3 jedi-0.19.1 js2py-0.74 keras-2.15.0 keras-tuner-1.4.7 kt-legacy-1.0.5 kubernetes-12.0.1 ml-dtypes-0.3.2 ml-metadata-1.15.0 ml-pipelines-sdk-1.15.1 objsize-0.7.0 orjson-3.10.7 overrides-7.7.0 pandas-1.5.3 portalocker-2.10.1 pyarrow-10.0.1 pyfarmhash-0.3.2 pyjsparser-2.7.1 pymongo-4.8.0 redis-5.0.8 requests-2.31.0 rouge-score-0.1.2 sacrebleu-2.4.3 scipy-1.12.0 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-data-validation-1.15.1 tensorflow-estimator-2.15.0 tensorflow-hub-0.15.0 tensorflow-model-analysis-0.46.0 tensorflow-serving-api-2.15.1 tensorflow-transform-1.15.0 tfx-1.15.1 tfx-bsl-1.15.1 uritemplate-3.0.1 wrapt-1.14.1 zstandard-0.23.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ba7bfbd4de0447ce94a9aebc360cf487",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install -U tfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFmd7HeRxtVS",
        "outputId": "b09f44c1-c343-4aa2-c17d-0a4bee098f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing modules/components.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile modules/components.py\n",
        "import os\n",
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "from tfx.components import(\n",
        "    CsvExampleGen,\n",
        "    StatisticsGen,\n",
        "    SchemaGen,\n",
        "    ExampleValidator,\n",
        "    Transform,\n",
        "    Trainer,\n",
        "    Evaluator,\n",
        "    Pusher\n",
        ")\n",
        "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.dsl.components.common.resolver import Resolver\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
        "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import(\n",
        "    LatestBlessedModelStrategy\n",
        ")\n",
        "\n",
        "\n",
        "def init_components(\n",
        "    data_dir,\n",
        "    transform_module,\n",
        "    training_module,\n",
        "    training_steps,\n",
        "    eval_steps,\n",
        "    serving_model_dir,\n",
        "):\n",
        "    \"\"\"\n",
        "    Initializes TFX components required for building a pipeline for training and deploying a model.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): The directory containing the input data.\n",
        "        transform_module (str): The path to the module containing the transformation logic.\n",
        "        training_module (str): The path to the module containing the training logic.\n",
        "        training_steps (int): The number of training steps.\n",
        "        eval_steps (int): The number of evaluation steps.\n",
        "        serving_model_dir (str): The directory where the trained model will be exported for serving.\n",
        "    \"\"\"\n",
        "\n",
        "    output = example_gen_pb2.Output(\n",
        "        split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n",
        "            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2)\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    example_gen = CsvExampleGen(\n",
        "        input_base=data_dir,\n",
        "        output_config=output\n",
        "    )\n",
        "\n",
        "    statistics_gen = StatisticsGen(\n",
        "        examples=example_gen.outputs['examples']\n",
        "    )\n",
        "\n",
        "    schema_gen = SchemaGen(\n",
        "        statistics=statistics_gen.outputs[\"statistics\"]\n",
        "    )\n",
        "\n",
        "    example_validator = ExampleValidator(\n",
        "        statistics=statistics_gen.outputs['statistics'],\n",
        "        schema=schema_gen.outputs['schema']\n",
        "    )\n",
        "\n",
        "    transform = Transform(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        module_file=os.path.abspath(transform_module)\n",
        "    )\n",
        "\n",
        "    trainer  = Trainer(\n",
        "        module_file=os.path.abspath(training_module),\n",
        "        examples = transform.outputs['transformed_examples'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        train_args=trainer_pb2.TrainArgs(splits=['train']),\n",
        "        eval_args=trainer_pb2.EvalArgs(splits=['eval'])\n",
        "    )\n",
        "\n",
        "    model_resolver = Resolver(\n",
        "        strategy_class=LatestBlessedModelStrategy,\n",
        "        model=Channel(type=Model),\n",
        "        model_blessing=Channel(type=ModelBlessing)\n",
        "    ).with_id('Latest_blessed_model_resolver')\n",
        "\n",
        "    slicing_specs=[\n",
        "        tfma.SlicingSpec(),\n",
        "        tfma.SlicingSpec(feature_keys=[\n",
        "            \"email\"\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    metrics_specs = [\n",
        "        tfma.MetricsSpec(metrics=[\n",
        "                tfma.MetricConfig(class_name='AUC'),\n",
        "                tfma.MetricConfig(class_name=\"Precision\"),\n",
        "                tfma.MetricConfig(class_name=\"Recall\"),\n",
        "                tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
        "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
        "                    threshold=tfma.MetricThreshold(\n",
        "                        value_threshold=tfma.GenericValueThreshold(\n",
        "                            lower_bound={'value':0.5}),\n",
        "                        change_threshold=tfma.GenericChangeThreshold(\n",
        "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                            absolute={'value':0.0001})\n",
        "                        )\n",
        "                )\n",
        "            ])\n",
        "    ]\n",
        "\n",
        "    eval_config = tfma.EvalConfig(\n",
        "        model_specs=[tfma.ModelSpec(label_key='label')],\n",
        "        slicing_specs=slicing_specs,\n",
        "        metrics_specs=metrics_specs\n",
        "    )\n",
        "\n",
        "    evaluator = Evaluator(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        model=trainer.outputs['model'],\n",
        "        baseline_model=model_resolver.outputs['model'],\n",
        "        eval_config=eval_config)\n",
        "\n",
        "    pusher = Pusher(\n",
        "        model=trainer.outputs[\"model\"],\n",
        "        model_blessing=evaluator.outputs[\"blessing\"],\n",
        "        push_destination=pusher_pb2.PushDestination(\n",
        "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "                base_directory=serving_model_dir\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    components = (\n",
        "        example_gen,\n",
        "        statistics_gen,\n",
        "        schema_gen,\n",
        "        example_validator,\n",
        "        transform,\n",
        "        trainer,\n",
        "        model_resolver,\n",
        "        evaluator,\n",
        "        pusher\n",
        "    )\n",
        "\n",
        "    return components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvT-loaO9-we",
        "outputId": "96c143b8-8f9f-42eb-be10-8ed8617ccadf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing modules/transform.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile modules/transform.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "LABEL_KEY = 'label'\n",
        "FEATURE_KEY = 'email'\n",
        "\n",
        "def transformed_name(key):\n",
        "  return key + \"_xf\"\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  outputs = {}\n",
        "  outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
        "  outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzvWCT5N-Kuw",
        "outputId": "b8ddffcc-1c34-4f86-acea-8da8b0475994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing modules/trainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile modules/trainer.py\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "\n",
        "LABEL_KEY = 'label'\n",
        "FEATURE_KEY = 'email'\n",
        "\n",
        "def transformed_name(key):\n",
        "    return key + \"_xf\"\n",
        "\n",
        "def gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "\n",
        "\n",
        "def input_fn(file_pattern,\n",
        "             tf_transform_output,\n",
        "             num_epochs,\n",
        "             batch_size=64)->tf.data.Dataset:\n",
        "\n",
        "    transform_feature_spec = (\n",
        "        tf_transform_output.transformed_feature_spec().copy())\n",
        "\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        num_epochs=num_epochs,\n",
        "        label_key = transformed_name(LABEL_KEY))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "SEQUENCE_LENGTH = 100\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "embedding_dim=16\n",
        "\n",
        "def model_builder():\n",
        "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
        "    reshaped_narrative = tf.reshape(inputs, [-1])\n",
        "    x = vectorize_layer(reshaped_narrative)\n",
        "    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name=\"embedding\")(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
        "\n",
        "    model.compile(\n",
        "        loss = 'binary_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "\n",
        "        feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "\n",
        "        return model(transformed_features)\n",
        "\n",
        "    return serve_tf_examples_fn\n",
        "\n",
        "def run_fn(fn_args: FnArgs) -> None:\n",
        "\n",
        "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
        "\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir = log_dir, update_freq='batch'\n",
        "    )\n",
        "\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "\n",
        "    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)\n",
        "    val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
        "    vectorize_layer.adapt(\n",
        "        [j[0].numpy()[0] for j in [\n",
        "            i[0][transformed_name(FEATURE_KEY)]\n",
        "                for i in list(train_set)]])\n",
        "\n",
        "    model = model_builder()\n",
        "\n",
        "\n",
        "    model.fit(x = train_set,\n",
        "            validation_data = val_set,\n",
        "            callbacks = [tensorboard_callback, es, mc],\n",
        "            steps_per_epoch = 1000,\n",
        "            validation_steps= 1000,\n",
        "            epochs=10)\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
        "                                    tf.TensorSpec(\n",
        "                                    shape=[None],\n",
        "                                    dtype=tf.string,\n",
        "                                    name='examples'))\n",
        "    }\n",
        "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B6p1tFXV-dES"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import Text\n",
        "\n",
        "from absl import logging\n",
        "from tfx.orchestration import metadata, pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "PIPELINE_NAME = \"email-classifier-pipeline\"\n",
        "\n",
        "# pipeline inputs\n",
        "DATA_ROOT = \"data\"\n",
        "TRANSFORM_MODULE_FILE = \"modules/transform.py\"\n",
        "TRAINER_MODULE_FILE = \"modules/trainer.py\"\n",
        "# requirement_file = os.path.join(root, \"requirements.txt\")\n",
        "\n",
        "# pipeline outputs\n",
        "OUTPUT_BASE = \"output\"\n",
        "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
        "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
        "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Tdf0iAH0-7TG"
      },
      "outputs": [],
      "source": [
        "def init_local_pipeline(\n",
        "    components, pipeline_root: Text\n",
        ") -> pipeline.Pipeline:\n",
        "\n",
        "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
        "    beam_args = [\n",
        "        \"--direct_running_mode=multi_processing\", # Add a space here\n",
        "        \"----direct_num_workers=0\"\n",
        "    ]\n",
        "\n",
        "    return pipeline.Pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        beam_pipeline_args=beam_args\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W926XDem_olA",
        "outputId": "6e081838-4aff-4f13-f803-3275ceb8d134"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733493   nanos: 641799688 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733493   nanos: 651441812 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733499   nanos: 351506710 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_9\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733508   nanos: 684129476 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733508   nanos: 693015336 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733512   nanos: 252401351 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_26\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-12\" \n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733535   nanos: 67982435 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733535   nanos: 76124668 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733541   nanos: 841496944 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_94\" transform_id: \"TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/readers.py:1086: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " email_xf (InputLayer)       [(None, 1)]               0         \n",
            "                                                                 \n",
            " tf.reshape (TFOpLambda)     (None,)                   0         \n",
            "                                                                 \n",
            " text_vectorization (TextVe  (None, 100)               0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 16)           160000    \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 16)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                1088      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 163201 (637.50 KB)\n",
            "Trainable params: 163201 (637.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "  16/1000 [..............................] - ETA: 7s - loss: 0.6799 - binary_accuracy: 0.5635"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_binary_accuracy improved from -inf to 0.52381, saving model to output/email-classifier-pipeline/Trainer/model/8/Format-Serving\n",
            "1000/1000 [==============================] - 3s 2ms/step - loss: 0.6674 - binary_accuracy: 0.5679 - val_loss: 0.6015 - val_binary_accuracy: 0.5238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733566   nanos: 392235279 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733566   nanos: 399148702 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724733573   nanos: 989243030 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_153\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
          ]
        }
      ],
      "source": [
        "from modules.components import init_components\n",
        "\n",
        "components = init_components(\n",
        "    DATA_ROOT,\n",
        "    training_module=TRAINER_MODULE_FILE,\n",
        "    transform_module=TRANSFORM_MODULE_FILE,\n",
        "    training_steps=5000,\n",
        "    eval_steps=1000,\n",
        "    serving_model_dir=serving_model_dir,\n",
        ")\n",
        "\n",
        "pipeline = init_local_pipeline(components, pipeline_root)\n",
        "BeamDagRunner().run(pipeline=pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7T4zeMotBVbE",
        "outputId": "53f3b62c-0dd6-4ee2-a795-4702ee0fd637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/output/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_schema/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_schema/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_schema/6/schema.pbtxt (deflated 55%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/.system/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/.system/executor_execution/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/.system/stateful_working_dir/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transformed_metadata/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transformed_metadata/schema.pbtxt (deflated 55%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transform_fn/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transform_fn/assets/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transform_fn/fingerprint.pb (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transform_fn/variables/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transform_fn/variables/variables.index (deflated 33%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transform_fn/variables/variables.data-00000-of-00001 (deflated 22%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/transform_fn/saved_model.pb (deflated 74%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/metadata/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transform_graph/6/metadata/schema.pbtxt (deflated 68%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/pre_transform_stats/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/pre_transform_stats/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/pre_transform_stats/6/FeatureStats.pb (deflated 68%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_stats/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_stats/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_stats/6/FeatureStats.pb (deflated 69%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/updated_analyzer_cache/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/updated_analyzer_cache/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/pre_transform_schema/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/pre_transform_schema/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/pre_transform_schema/6/schema.pbtxt (deflated 68%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_anomalies/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_anomalies/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/post_transform_anomalies/6/SchemaDiff.pb (deflated 34%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transformed_examples/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transformed_examples/6/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transformed_examples/6/Split-train/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transformed_examples/6/Split-train/transformed_examples-00000-of-00001.gz (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transformed_examples/6/Split-eval/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Transform/transformed_examples/6/Split-eval/transformed_examples-00000-of-00001.gz (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/.system/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/.system/executor_execution/7/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/.system/stateful_working_dir/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/anomalies/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/anomalies/7/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/anomalies/7/Split-train/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/anomalies/7/Split-train/SchemaDiff.pb (deflated 40%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/anomalies/7/Split-eval/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/ExampleValidator/anomalies/7/Split-eval/SchemaDiff.pb (deflated 40%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/executor_execution/3/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/executor_execution/1/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/driver_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/driver_execution/1724733487556509/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/driver_execution/1724733441882117/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/stateful_working_dir/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/.system/stateful_working_dir/b6a2d8cd-9cdf-4f51-a1d5-96113e9ae9c9/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/examples/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/examples/3/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/examples/3/Split-train/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/examples/3/Split-train/data_tfrecord-00000-of-00001.gz (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/examples/3/Split-eval/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/CsvExampleGen/examples/3/Split-eval/data_tfrecord-00000-of-00001.gz (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/.system/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/.system/executor_execution/8/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/.system/stateful_working_dir/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model_run/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model_run/8/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/Format-Serving/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/Format-Serving/assets/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/Format-Serving/fingerprint.pb (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/Format-Serving/variables/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/Format-Serving/variables/variables.index (deflated 59%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/Format-Serving/variables/variables.data-00000-of-00001 (deflated 68%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/Format-Serving/keras_metadata.pb (deflated 85%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/Format-Serving/saved_model.pb (deflated 86%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/logs/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/logs/validation/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/logs/validation/events.out.tfevents.1724733551.2346a2e77275.1906.1.v2 (deflated 36%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/logs/train/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Trainer/model/8/logs/train/events.out.tfevents.1724733550.2346a2e77275.1906.0.v2 (deflated 86%)\n",
            "  adding: content/output/email-classifier-pipeline/metadata.sqlite (deflated 94%)\n",
            "  adding: content/output/email-classifier-pipeline/Pusher/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Pusher/.system/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Pusher/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Pusher/.system/executor_execution/10/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Pusher/.system/stateful_working_dir/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Pusher/pushed_model/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Pusher/pushed_model/10/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/SchemaGen/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/SchemaGen/.system/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/SchemaGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/SchemaGen/.system/executor_execution/5/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/SchemaGen/.system/stateful_working_dir/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/SchemaGen/schema/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/SchemaGen/schema/5/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/SchemaGen/schema/5/schema.pbtxt (deflated 68%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/.system/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/.system/executor_execution/9/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/.system/stateful_working_dir/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/blessing/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/blessing/9/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/blessing/9/NOT_BLESSED (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/evaluation/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/evaluation/9/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/evaluation/9/plots-00000-of-00001.tfrecord (deflated 54%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/evaluation/9/attributions-00000-of-00001.tfrecord (deflated 54%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/evaluation/9/metrics-00000-of-00001.tfrecord (deflated 79%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/evaluation/9/eval_config.json (deflated 63%)\n",
            "  adding: content/output/email-classifier-pipeline/Evaluator/evaluation/9/validations.tfrecord (deflated 69%)\n",
            "  adding: content/output/email-classifier-pipeline/_wheels/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/_wheels/tfx_user_code_Trainer-0.0+8a261f3e647d816c7f3a74a86570b51ed296b4f8dc056c41c7e6bc0a58150fbc-py3-none-any.whl (deflated 20%)\n",
            "  adding: content/output/email-classifier-pipeline/_wheels/tfx_user_code_Transform-0.0+8a261f3e647d816c7f3a74a86570b51ed296b4f8dc056c41c7e6bc0a58150fbc-py3-none-any.whl (deflated 20%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/.system/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/.system/executor_execution/4/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/.system/stateful_working_dir/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/statistics/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/statistics/4/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/statistics/4/Split-train/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/statistics/4/Split-train/FeatureStats.pb (deflated 68%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/statistics/4/Split-eval/ (stored 0%)\n",
            "  adding: content/output/email-classifier-pipeline/StatisticsGen/statistics/4/Split-eval/FeatureStats.pb (deflated 70%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4425059b-a286-48ab-9256-d710bc4b9f6a\", \"output.zip\", 787982)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/output.zip /content/output\n",
        "files.download('/content/output.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr01kBDNJAx_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
